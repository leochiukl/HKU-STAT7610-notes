\section{Measure Theory III --- Measurable Functions}
\label{sect:meas-fn}
\begin{enumerate}
\item Apart from the \emph{measure} itself, another important notion in measure
theory is \emph{measurable function}, which maps between different measurable
spaces, providing ``connections'' between them. A specific kind of measurable
function of interest is the \emph{random variable} (which should be familiar to
you \faIcon[regular]{grin-wink}, but perhaps you have not seen its
\emph{formal} definition), which quantifies sample points in the sample spaces,
or more abstractly, maps from the measurable space \((\Omega,\mathcal{F})\)
(for sample points) to the measurable space \((\R,\mathcal{B}(\R))\) (for real
numbers quantifying sample points).
\end{enumerate}
\subsection{Measurable Functions}
\begin{enumerate}
\item \textbf{Definition of measurable functions and random
variables/vectors/sequences.} Let \((\Omega,\mathcal{F})\) and
\((\Omega',\mathcal{F}')\) be measurable spaces.  Then a function
\(X:\Omega\to\Omega'\) is \defn{\((\mathcal{F},\mathcal{F}')\)-measurable} (or
just \defn{measurable}) if \(X^{-1}(\mathcal{F}')\subseteq \mathcal{F}\), i.e.,
\(X^{-1}(A')\in\mathcal{F}\) for all \(A'\in\mathcal{F}'\).

\begin{intuition}
The \((\mathcal{F},\mathcal{F}')\)-measurability suggests the ``compatibility''
of \(X\) with both \(\mathcal{F}\) and \(\mathcal{F}'\): Picking any set
\(A'\in\mathcal{F}'\), we can assign measure to the preimage \(X^{-1}(A')\) as
it is in \(\mathcal{F}\). This property is analogous to the \emph{continuity}
of a function: Let \((\Omega,\mathcal{T})\) and \((\Omega',\mathcal{T}')\) be
topological spaces. Then \(X:\Omega\to\Omega'\) is \defn{continuous} if
\(X^{-1}(\mathcal{T}')\subseteq \mathcal{T}\), or in words, the preimage of
every open subset of \(\Omega'\) under \(X\) is open in \(\Omega\).
\end{intuition}

\emph{Special cases:}
\begin{itemize}
\item If \((\Omega',\mathcal{F}')=(\R,\mathcal{B}(\R))\), then
every \((\mathcal{F},\mathcal{F}')\)-measurable function \(X\) is called a
\defn{random variable}.
\item If \((\Omega',\mathcal{F}')=(\R^d,\mathcal{B}(\R^d))\) for some
\(d\in\N\), then every \((\mathcal{F},\mathcal{F}')\)-measurable function \(X\)
is called a \defn{random vector}.
\item If \((\Omega',\mathcal{F}')=(\R^{\N},\mathcal{B}(\R^{\N}))\), then every
\((\mathcal{F},\mathcal{F}')\)-measurable function  \(X\) is called a
\defn{random sequence}.
\begin{note}
\(\R^{\N}\) denotes the set of all functions from \(\N\) to \(\R\), i.e., all
real-valued sequences. Sometimes we write it as \(\R^{\infty}\), and in the
following, the notation ``\(\R^d\)'' would also include this case
(\(d=\infty\)), unless otherwise specified.
\end{note}
\end{itemize}
\begin{remark}
\item Every \((\mathcal{F},\mathcal{B}(\R^d))\)-measurable function \(X\) is
also said to be \defn{\(\mathcal{F}\)-measurable}, denoted by
\defn{\(X\in\mathcal{F}\)} (this does not mean that \(X\) is an element in
\(\mathcal{F}\) \warn{}).
\item \((\mathcal{B}(\Omega),\mathcal{B}(\R^d))\)-measurable and
\((\bar{\mathcal{B}}(\Omega),\mathcal{B}(\R^d))\)-measurable functions are said to be
\defn{Borel measurable} and \defn{Lebesgue measurable} respectively.
\item \emph{(completeness of \(\mathcal{F}\))} To enrich the collection of
measurable functions, the \(\mathcal{F}\) chosen is usually the completion of a
\(\sigma\)-algebra (i.e., \(\mathcal{F}=\bar{\mathcal{F}}\); see
\Cref{thm:complete-meas}), which comprises of more sets after the completion,
so that it contains \(X^{-1}(\mathcal{F}')\) for more functions \(X\), thereby
making more functions measurable.

On the other hand, the \(\mathcal{F}'\) chosen is often not a completion. For
example, in the definition of random variable/vector/sequence, we set
\(\mathcal{F}'\) to be the Borel \(\sigma\)-algebra \(\mathcal{B}(\R^d)\)
rather than the Lebesgue \(\sigma\)-algebra \(\bar{\mathcal{B}}(\R^d)\) (the
completion of \(\mathcal{B}(\R^d)\)).  The reason is that by including fewer
sets in \(\mathcal{F}'\), there are less opportunities for the preimage
\(X^{-1}(A')\) to fall outside \(\mathcal{F}\), which again allows more
functions to be measurable.
\end{remark}
\item \textbf{Examples of non-measurable and measurable functions.} To
understand the concept of measurable function better, we study some examples of
non-measurable and measurable functions. As you will see, the choice of
\(\mathcal{F}\) is quite important for the measurability.

\emph{Examples of non-measurable functions:}
\begin{enumerate}[label={(\arabic*)}]
\item Let \(\Omega\) be a nonempty set and
\(\mathcal{F}=\{\varnothing,\Omega\}\) be the trivial \(\sigma\)-algebra on
\(\Omega\). Then every non-constant function \(X:\Omega\to\R\) is
not \(\mathcal{F}\)-measurable.

\begin{pf}
Fix any \(x\in\ran{X}=\{X(\omega):\omega\in\Omega\}\). Note that
\(\{x\}=\bigcap_{n=1}^{\infty}(x-1/n,x]\in\mathcal{B}(\R)\), but
\(X^{-1}(\{x\})\) is neither \(\varnothing\) (as \(x\in\ran{X}\)) nor
\(\Omega\) (as \(X\) is non-constant, so \(\ran{X}\) contains values other than
\(x\)). Thus \(X^{-1}(\{x\})\notin\mathcal{F}\).
\end{pf}
\item Let \((\Omega,\mathcal{F})\) be a measurable space, and \(V\) be a
non-measurable subset of \(\Omega\) (i.e., \(V\subseteq \Omega\) but
\(V\notin\mathcal{F}\))), e.g., the Vitali set for the case
\((\Omega,\mathcal{F})=(\R,\mathcal{B}(\R))\). \begin{note} By
\Cref{thm:vitali}, it is not possible to define the Lebesgue measure on the
Vitali set \(V\), which implies that \(V\notin\bar{\mathcal{B}}(\R)\), as the
domain of Lebesgue measure is \(\bar{\mathcal{B}}(\R)\). Thus we have
\(V\notin\mathcal{B}(\R)\). \end{note}

Then, \(X:\Omega\to\R\) with \(X=\indic_{V}\) is not
\(\mathcal{F}\)-measurable.

\begin{pf}
Note that \(\{1\}\in\mathcal{B}(\R)\) but \(X^{-1}(\{1\})=V\notin\mathcal{F}\).
\end{pf}
\end{enumerate}
\emph{Examples of measurable functions:}
\begin{enumerate}[label={(\arabic*)}]
\item Let \((\Omega,\mathcal{F})\) be a measurable space. Then every constant
function \(X:\Omega\to\R\) with \(X\equiv c\) is \(\mathcal{F}\)-measurable.

\begin{pf}
For every \(B\in\mathcal{B}(\R)\), we have
\[X^{-1}(B)=\begin{cases}
\Omega&\text{if \(B\) contains \(c\),} \\
\varnothing&\text{if \(B\) does not contain \(c\).}
\end{cases}
\] Hence, we have
\(X^{-1}(\mathcal{B}(\R))=\{\varnothing,\Omega\}\subseteq \mathcal{F}\) always.
\end{pf}
\item Let \((\Omega,\mathcal{F})\) be a measurable space, and
\(A\in\mathcal{F}\). Then \(X:\Omega\to\R\) with \(X=\indic_{A}\) is
\(\mathcal{F}\)-measurable.

\begin{pf}
For every \(B\in\mathcal{B}(\R)\), we have
\[
X^{-1}(B)=\begin{cases}
\Omega&\text{if \(B\) contains \(0\) and \(1\),} \\
A&\text{if \(B\) contains \(1\) but not \(0\),} \\
A^c&\text{if \(B\) contains \(0\) but not \(1\),} \\
\varnothing&\text{if \(B\) contains none of \(0\) and \(1\).}
\end{cases}
\]
Hence, \(X^{-1}(\mathcal{B}(\R))=\{\varnothing,A,A^c,\Omega\}\subseteq \mathcal{F}\),
since \(A\in\mathcal{F}\).
\end{pf}
\item \emph{(simple random variables)} Let \((\Omega,\mathcal{F})\) be a
measurable space, and \(A_1,\dotsc,A_n\in\mathcal{F}\) be pairwise disjoint
sets.  Then, \(X:\Omega\to\R\) with \(X=\sum_{i=1}^{n}x_i\indic_{A_i}\) for
some \(x_1,\dotsc,x_n\in\R\) is \(\mathcal{F}\)-measurable. \begin{note}
Such function \(X\) is known as a \defn{simple function}, which is
essential for the construction of \emph{Lebesgue integral} (see
\Cref{sect:integration-expectation}).
\end{note}

\begin{pf}
Similar to the previous example, we have
\(X^{-1}(\mathcal{B}(\R))=\{\biguplus_{i\in I}^{}A_i:I\subseteq
\{1,\dotsc,n\}\}\subseteq \mathcal{F}\).
\end{pf}
\end{enumerate}
\item \textbf{Properties of measurable functions.} After studying some examples
of non-measurable and measurable functions, we will prove some properties of
measurable functions that facilitate our checking of measurability. The
following lemma will be helpful for the proof:
\begin{lemma}[Commutativity of \(\sigma(\cdot)\) and \(X^{-1}(\cdot)\)]
\label{lma:sig-alg-gen-preimg-comm}
Let \((\Omega,\mathcal{F})\) and \((\Omega',\mathcal{F}')\) be measurable
spaces, \(X:\Omega\to\Omega'\) be a function, and \(\mathcal{A}'\subseteq
\mathcal{P}(\Omega')\). Then we have
\(\sigma(X^{-1}(\mathcal{A}'))=X^{-1}(\sigma(\mathcal{A}'))\).
\begin{note}
In particular, if \(\mathcal{F}'=\sigma(\mathcal{A}')\), then
\(\sigma(X^{-1}(\mathcal{A}'))
=X^{-1}(\sigma(\mathcal{A}'))=X^{-1}(\mathcal{F}')\overset{\text{(definition)}}{=}\sigma(X)\).
\end{note}
\end{lemma}
\begin{pf}
``\(\subseteq\)'': Since \(\mathcal{A}'\subseteq \sigma(\mathcal{A}')\), by
\labelcref{it:preimg-prop} we have \(X^{-1}(\mathcal{A}')\subseteq
X^{-1}(\sigma(\mathcal{A}'))\).  As \(\sigma(\mathcal{A}')\) is a
\(\sigma\)-algebra on \(\Omega'\), \(X^{-1}(\sigma(\mathcal{A}'))\) is a
\(\sigma\)-algebra on \(\Omega\) (namely the preimage \(\sigma\)-algebra; see
\labelcref{it:preimg-sig-alg}), which contains \vc{\(X^{-1}(\mathcal{A}')\)} as
shown previously. Thus, \(\sigma(\vc{X^{-1}(\mathcal{A}')})\subseteq X^{-1}(\sigma(\mathcal{A}'))\)
by the minimality.

``\(\supseteq\)'': Let \(\mathcal{G}':=\{A'\subseteq \Omega':
X^{-1}(A')\in\sigma(X^{-1}(\mathcal{A}'))\}\). We first show that
\(\mathcal{G}'\) is a \(\sigma\)-algebra on \(\Omega'\):
\begin{enumerate}[label={(\arabic*)}]
\item \(\Omega'\in\mathcal{G}'\) since \(X^{-1}(\Omega')=\Omega\in\sigma(X^{-1}(\mathcal{A}'))\).
\item Fix any \(A'\in\mathcal{G}'\). Then we have \(A'\subseteq \Omega'\) with
\(X^{-1}(A')\in\sigma(X^{-1}(\mathcal{A}'))\). So we know \(A'^c\subseteq \Omega'\).
Also,
\[
X^{-1}(A'^c)\overset{\labelcref{it:preimg-prop}}{=}
X^{-1}(A')^c\overset{\text{(closed under complementation)}}{\in}
\sigma(X^{-1}(\mathcal{A}')).
\]
Thus, \(A'^c\in\mathcal{G}'\).
\item Fix any \(A_1',A_2',\dotsc\in\mathcal{G}'\). Then we
have \(A_i'\subseteq \Omega'\) with
\(X^{-1}(A_i')\in\sigma(X^{-1}(\mathcal{A}'))\) for all \(i\in\N\). Therefore,
\(\bigcup_{i=1}^{\infty}A_i'\subseteq \Omega'\) and
\[
X^{-1}\left(\bigcup_{i=1}^{\infty}A_i'\right)\overset{\labelcref{it:preimg-prop}}{=}
\bigcup_{i=1}^{\infty}X^{-1}(A_i')\overset{\text{(closed under countable unions)}}{\in}
\sigma(X^{-1}(\mathcal{A}')),
\]
meaning that \(\bigcup_{i=1}^{\infty}A_i'\in\mathcal{G}'\).
\end{enumerate}
Note that \(X^{-1}(A')\in\sigma(X^{-1}(\mathcal{A}'))\) for all
\(A'\in\mathcal{A}'\), so \(\mathcal{A}'\subseteq \mathcal{G}'\). As
\(\mathcal{G}'\) is a \(\sigma\)-algebra, we then have
\(\vc{\sigma(\mathcal{A}')}\subseteq \vc{\mathcal{G}'}\), which implies by
\labelcref{it:preimg-prop} that \(X^{-1}(\vc{\sigma(\mathcal{A}')})\subseteq X^{-1}(\vc{\mathcal{G}'})
\underset{\text{(definition of \(\mathcal{G}'\))}}{\subseteq}\sigma(X^{-1}(\mathcal{A}'))\).
\end{pf}

With the help of \Cref{lma:sig-alg-gen-preimg-comm}, we are now ready to prove
the following properties of measurable functions.
\begin{enumerate}
\item\label{it:check-meas-via-preimg-gen} \emph{(checking measurability by
considering preimages of generator)} Let \((\Omega,\mathcal{F})\) and
\((\Omega',\mathcal{F}')\) be measurable spaces, and \(\mathcal{A}'\subseteq
\mathcal{P}(\Omega')\) be a \emph{generator} of \(\mathcal{F}'\), i.e.,
\(\sigma(\mathcal{A}')=\mathcal{F}'\). Then, \(X:\Omega\to\Omega'\) is
measurable iff \(X^{-1}(\mathcal{A}')\subseteq \mathcal{F}\).

\begin{pf}
``\(\Rightarrow\)'': Since \(\mathcal{A}'\subseteq
\sigma(\mathcal{A}')=\mathcal{F}\), by \labelcref{it:preimg-prop} we have
\(X^{-1}(\mathcal{A}')\subseteq X^{-1}(\mathcal{F}')\overset{\text{(\(X\)
measurable)}}{\subseteq}\mathcal{F}\).

``\(\Leftarrow\)'': Assume that \(X^{-1}(\mathcal{A}')\subseteq \mathcal{F}\).
Since \(\mathcal{F}\) is a \(\sigma\)-algebra, by the minimality we have
\(\sigma(X^{-1}(\mathcal{A}'))\subseteq \mathcal{F}\). Thus, we have
\(
X^{-1}(\mathcal{F}')=X^{-1}(\sigma(\mathcal{A'}))
\underset{\text{(\Cref{lma:sig-alg-gen-preimg-comm})}}{=}
\sigma(X^{-1}(\mathcal{A}'))
\subseteq \mathcal{F}
\).
\end{pf}
\item \label{it:meas-compo-meas} \emph{(composition of measurable functions is
measurable)} Let \((\Omega,\mathcal{F})\), \((\Omega',\mathcal{F}')\), and
\((\Omega'',\mathcal{F}'')\) be measurable spaces. If \(X:\Omega\to\Omega'\) and
\(Y:\Omega'\to\Omega''\) are \((\mathcal{F},\mathcal{F}')\)-measurable and
\((\mathcal{F}',\mathcal{F}'')\)-measurable respectively, then the composition
\(Y\circ X:\Omega\to\Omega''\) is \((\mathcal{F},\mathcal{F}'')\)-measurable.

\begin{pf}
For all \(A''\in\mathcal{F}''\), we have
\begin{align*}
(Y\circ X)^{-1}(A'')
&=\{\omega\in\Omega:Y(\vc{X(\omega)})\in A''\}
\overset{\left(Y(\vc{\omega'})\in A''\iff \vc{\omega'}\in Y^{-1}(A'')\right)}{=}
\{\omega\in\Omega:\vc{X(\omega)}\in \orc{Y^{-1}(A'')}\} \\
\overset{\left(X(\omega)\in \orc{A'}\iff \omega\in X^{-1}(\orc{A'})\right)}&{=}
\{\omega\in\Omega:\omega\in X^{-1}(\orc{Y^{-1}(A'')})\}
=X^{-1}(\underbrace{Y^{-1}(A'')}_{\mathclap{\in\mathcal{F}'\text{ as \(Y\) is measurable}}})
\overset{\text{(\(X\) measurable)}}{\in}\mathcal{F}.
\end{align*}
\end{pf}
\item \label{it:cts-fn-meas} \emph{(continuous functions are measurable)} Let
\((\Omega,\mathcal{T})\) and \((\Omega',\mathcal{T}')\) be topological spaces.
If \(X:\Omega\to\Omega'\) is continuous, then \(X\) is
\((\mathcal{B}(\Omega),\mathcal{B}(\Omega'))\)-measurable.

\begin{pf}
By the continuity of \(X\), we have \(X^{-1}(\mathcal{T}')\subseteq
\mathcal{T}\subseteq
\sigma(\mathcal{T})\overset{\text{(definition)}}{=}\mathcal{B}(\Omega)\).  As
\(\mathcal{B}(\Omega)\) is a \(\sigma\)-algebra, by the minimality we have
\(\sigma(X^{-1}(\mathcal{T}'))\subseteq \mathcal{B}(\Omega)\). Noting that
\(\sigma(\mathcal{T}')=\mathcal{B}(\Omega')\) by definition, using
\Cref{lma:sig-alg-gen-preimg-comm} gives
\(X^{-1}(\mathcal{B}(\Omega'))=X^{-1}(\sigma(\mathcal{T}'))=\sigma(X^{-1}(\mathcal{T}'))\subseteq
\mathcal{B}(\Omega)\), as desired.
\end{pf}
\item\label{it:mono-fn-meas} \emph{(monotone functions are measurable)}
If \(h:\R\to\R\) is a monotone (increasing or decreasing) function, then \(h\) is
\((\mathcal{B}(\R),\mathcal{B}(\R))\)-measurable.

\begin{pf}
WLOG, we prove only the case where \(h\) is increasing. By
\Cref{prp:borel-rd-generators}, we know \(\{[t,\infty):t\in\R\}\) is a generator of \(\mathcal{B}(\R)\).
Thus by \labelcref{it:check-meas-via-preimg-gen} it suffices to show that
\(h^{-1}([t,\infty))\in\mathcal{B}(\R)\) for all \(t\in\R\).

Fix any \(t\in\R\) and consider the set
\(A_t:=h^{-1}([t,\infty))=\{x\in\R:h(x)\ge t\}\).  Fix any \(x_1\in A_t\).
Then, for every \(x_2\ge x_1\), we have \(h(x_2) \overset{\text{(\(h\)
increasing)}}{\ge}h(x_1)\overset{(x_1\in A_t)}{\ge}t\), thus \(x_2\in A_t\).
This forces \(A_t\) to be of the form (i) \([\inf A_t,\infty)\) if
\(t\in\ran{h}\), or (ii) \((\inf A_t,\infty)\) if \(h(\inf A_t)<t\). In either
case, \(A_t\) is in \(\mathcal{B}(\R)\), as desired.
\end{pf}
\end{enumerate}
\item\label{it:lebesgue-set-not-borel-set} \textbf{An example of Lebesgue set
that is not a Borel set.} Now we have enough tools to construct a Lebesgue set
that is not a Borel set, whose existence is asserted in
\labelcref{it:lebesgue-stieltjes-meas}.

Let \(U:=F^{-}(V)\), where \(F^{-}\) is the quantile function of the
\emph{Cantor distribution function} \(F\) (recall
\labelcref{it:cts-df-not-abs-cts}), and \(V\) is the Vitali set.
\begin{note}
Here \(F^{-}(V)\) denotes the image set \(\{F^{-}(y):y\in V\}\).
\end{note}
We claim that \(U\) is a Lebesgue set but not a Borel set.

\begin{pf}
\textbf{Showing that \(U\) is a Lebesgue set.} Since the Cantor set
\(\mathcal{C}\) is a countable intersection of closed sets (complements of open
sets, which are in \(\mathcal{B}(\R)=\sigma(\mathcal{T})\)), we know
\(\mathcal{C}\in\mathcal{B}(\R)\).  Also, we have previously shown in
\labelcref{it:lebesgue-null-sets-eg} that \(\lambda(\mathcal{C})=0\), so
\(\mathcal{C}\) is a \(\lambda\)-null set in \(\mathcal{B}(\R)\). 

Noting that \(V\subseteq [0,1]\), we have \(U=F^{-}(V)\subseteq
F^{-}([0,1])=\mathcal{C}\).  Therefore, \(U\) belongs to the Lebesgue
\(\sigma\)-algebra \(\bar{\mathcal{B}}(\R)\) by the definition of completion,
and is thus a Lebesgue set.

\textbf{Showing that \(U\) is not a Borel set.}
By \labelcref{it:gen-inv-prop}, we know \(F^{-}:[0,1]\to\eR\) is increasing.
By defining \(F^{-}(y):=F^{-}(0)\) for all \(y<0\) and \(F^{-}(y):=F^{-}(1)\) for all \(y>1\),
we can extend the domain of \(F^{-}\) to \(\R\), and we shall consider this
extended \(F^{-}\) in the following. Note that \(F^{-}\) is still increasing
after the extension, so \(F^{-}\) is Borel measurable by \labelcref{it:mono-fn-meas}.

Now assume to the contrary that \(U\) is a Borel set. Then since \(F^{-}\) is
Borel measurable, the preimage \((F^{-})^{-1}(U)\) would belong to
\(\mathcal{B}(\R)\). However, we note that
\begin{align*}
(F^{-})^{-1}(U)&=\{y\in\R:F^{-}(y)\in U\}
=\{y\in\R:F^{-}(y)\in F^{-}(V)\} \\
\overset{\left(y\in V\iff  F^{-}(y)\in F^{-}(V)\right)}&{=}
\{y\in\R:y\in V\}=V\notin\mathcal{B}(\R),
\end{align*}
contradiction.
\end{pf}
\item \textbf{Properties of random variables/vectors.} After studying
properties of general measurable functions, we now investigate the properties
of more specific measurable functions, namely random variables/vectors.
\begin{enumerate}
\item \label{it:ran-vec-vec-rvs} \emph{(random vectors are vectors of random
variables)} Let \((\Omega,\mathcal{F})\) be a measurable space and let
\(X:\Omega\to\R^d\) be a function. Then \(X\) is a random vector iff \(X=(X_1,\dotsc,X_d)\)
for some random variables \(X_1,\dotsc,X_d\).
\begin{note}
In view of this result, often we use bold notation to denote a random vector,
e.g., \(\vect{X}\).
\end{note}

\begin{pf}
``\(\Rightarrow\)'': Assume \(X\) is a random vector, thus measurable. Fix any
\(j=1,\dotsc,d\), and consider the projection \(\pi_j:\R^d\to\R\) defined by
\(\pi_j(x_1,\dotsc,x_d)=x_j\). Note that \(\pi_j\) is continuous since for all
\(\varepsilon>0\), we can choose \(\delta=\varepsilon>0\) such that, whenever
\(\|\vect{x}-\vect{y}\|<\delta\), we have
\(|\pi_{j}(\vect{x})-\pi_j(\vect{y})|=|x_j-y_j|\le\|\vect{x}-\vect{y}\|
<\delta=\varepsilon\).
Thus, by \labelcref{it:cts-fn-meas}, \(\pi_j\) is measurable, and so
 \(\pi_j\circ X:\Omega\to\R\) is measurable by \labelcref{it:meas-compo-meas},
hence a random variable. By letting \(X_j:=\pi_j\circ X\) for all
\(j=1,\dotsc,d\), we then have \(X=(X_1,\dotsc,X_d)\).

``\(\Leftarrow\)'': Assume \(X=(X_1,\dotsc,X_d)\) for some random variables
\(X_1,\dotsc,X_d\).  By \Cref{prp:borel-rd-generators} we know
\(\{(\vect{a},\vect{b}]:\vect{a}<\vect{b}\}\) is a generator of
\(\mathcal{B}(\R^d)\), thus by \labelcref{it:check-meas-via-preimg-gen} it
suffices to show that \(X^{-1}((\vect{a},\vect{b}])\in\mathcal{F}\) for all
\(\vect{a}<\vect{b}\): For all
\((a_1,\dotsc,a_d)=\vect{a}<\vect{b}=(b_1,\dotsc,b_d)\), we have
\[
X^{-1}((\vect{a},\vect{b}])
=\{\omega\in\Omega:X_j(\omega)\in(a_j,b_j]\text{ for all \(j=1,\dotsc,d\)}\}
=\bigcap_{j=1}^{d}\underbrace{X_j^{-1}((a_j,b_j])}_{\in\mathcal{F}\text{ as \(X_j\in\mathcal{F}\)}}
\in\mathcal{F}.
\]
\end{pf}
\item \label{it:meas-fn-ran-vec} \emph{(measurable functions of random vectors
are random vectors)} Let \(\vect{h}:\R^d\to\R^k\) be a measurable function. If
\(\vect{X}:\Omega\to\R^d\) is a random vector, then
\(\vect{h}(\vect{X}):\Omega\to\R^k\) is a random vector.

\begin{note}
Particularly, since sums, products, minima, and maxima are continuous functions
and thus measurable, sums, products, minima, and maxima of random variable are
random variables.
\end{note}

\begin{pf}
It follows from \labelcref{it:meas-compo-meas}.
\end{pf}
\item \label{it:seq-rvs-meas} \emph{(measurability about sequences of random variables)}
Let \(\{X_{i}\}_{i\in\N}\) be a sequence of random variables on a measurable
space \((\Omega,\mathcal{F})\). \begin{note}
It can be shown that \(X\) is a random sequence iff \(X=\{X_{i}\}_{i\in\N}\)
for some random variables \(X_1,X_2,\dotsc\). So the sequence here indeed
constitutes a random sequence.
\end{note}
\begin{enumerate}
\item \(\inf_{k\ge n}X_k\), \(\sup_{k\ge n}X_k\), \(\liminf_{n\to \infty}X_n
:=\sup_{n\ge 1}(\inf_{k\ge n}X_k)\), and \\
\(\limsup_{n\to\infty}X_n:=\inf_{n\ge 1}(\sup_{k\ge n}X_k)\) are all random
variables.

\begin{note}
``\(\inf_{k\ge n}X_k\)'' refers to the function that takes the value
\(\inf_{k\ge n}X_k(\omega)=\inf\{X_k(\omega):k\ge n\}\) with input
\(\omega\in\Omega\); similar for others.
\end{note}
\item If \(\lim_{n\to\infty}X_n(\omega)\) exists for all \(\omega\in\Omega\),
then \(\lim_{n\to\infty}X_n\) is a random variable.

\begin{note}
Again, ``\(\lim_{n\to\infty}X_n\)'' refers to the function that takes the value
\(\lim_{n\to\infty}X_n(\omega)\) with input \(\omega\in\Omega\).
\end{note}
\item The set \(\{\omega\in\Omega:\lim_{n\to\infty}X_n(\omega)\text{ exists}\}\) is measurable 
(i.e., is in \(\mathcal{F}\)).
\end{enumerate}

\begin{pf}
\begin{enumerate}
\item 
\begin{itemize}
\item \emph{\(\inf_{k\ge n}X_k\) is a random variable:}
For all \(x\in\R\),
\begin{align*}
\left(\inf_{k\ge n}X_k\right)^{-1}((-\infty,x])
&=\left\{\omega\in\Omega:\inf_{k\ge n}X_k(\omega)\le x\right\}
=\bigcup_{k\ge n}^{}\{\omega\in\Omega:X_k(\omega)\le x\} \\
&=\bigcup_{k\ge n}^{}\underbrace{X_k^{-1}((-\infty,x])}_{\in\mathcal{F}\text{ as \(X_k\in\mathcal{F}\)}}
\in\mathcal{F}.
\end{align*}
Thus the result follows by \Cref{prp:borel-rd-generators} and
\labelcref{it:check-meas-via-preimg-gen}.
\item \emph{\(\sup_{k\ge n}X_k\) is a random variable:}
For all \(x\in\R\),
\begin{align*}
\left(\sup_{k\ge n}X_k\right)^{-1}([x,\infty))
&=\left\{\omega\in\Omega:\sup_{k\ge n}X_k(\omega)\ge x\right\}
=\bigcup_{k\ge n}^{}\{\omega\in\Omega:X_k(\omega)\ge x\} \\
&=\bigcup_{k\ge n}^{}\underbrace{X_k^{-1}([x,\infty))}_{\in\mathcal{F}\text{ as \(X_k\in\mathcal{F}\)}}
\in\mathcal{F}.
\end{align*}
Thus the result follows by \Cref{prp:borel-rd-generators} and
\labelcref{it:check-meas-via-preimg-gen}.
\item \emph{\(\liminf_{n\to\infty}X_k\) and \(\limsup_{n\to\infty}X_k\) are random variables:}
It follows from writing \(\liminf_{n\to \infty}X_n
=\sup_{n\ge 1}(\inf_{k\ge n}X_k)\) and \(\limsup_{n\to\infty}X_n=\inf_{n\ge 1}(\sup_{k\ge n}X_k)\),
and applying \labelcref{it:meas-compo-meas}.
\end{itemize}
\item Since \(\lim_{n\to\infty}X_n(\omega)\) exists for all \(\omega\in\Omega\),
we have \(\lim_{n\to\infty}X_n(\omega)=\limsup_{n\to\infty}X_n(\omega)\)
for all \(\omega\in\Omega\). So, the result follows from (i).
\item Note that for all \(\omega\in\Omega\),
\(\liminf_{n\to\infty}X_n(\omega)\le\limsup_{n\to\infty}X_n(\omega)\)
and the limit \(\lim_{n\to\infty}X_n(\omega)\) exists iff the equality is
achieved. Hence, we can write
\begin{align}
\nonumber\{\omega\in\Omega:\lim_{n\to\infty}X_n(\omega)\text{ exists}\}^{c}
\label{eq:q-liminf-limsup}&=\left\{\omega\in\Omega:\liminf_{n\to\infty}X_n(\omega)<\limsup_{n\to\infty}X_n(\omega)\right\} \\
&=\bigcup_{q\in\Q}^{}\left\{\omega\in\Omega:\liminf_{n\to\infty}X_n(\omega)<
q<\limsup_{n\to\infty}X_n(\omega)\right\} \\
\nonumber&=
\bigcup_{q\in\Q}^{}
\Bigg(\underbrace{\left\{\omega\in\Omega:\liminf_{n\to\infty}X_n(\omega)< q\right\}}_{\in\mathcal{F}\text{ by (i)}}
\cap\underbrace{\left\{\omega\in\Omega:\limsup_{n\to\infty}X_n(\omega)>q\right\}}_{\in\mathcal{F}
\text{ by (i)}}
\Bigg) \\
\nonumber\overset{\text{(\(\Q\) countable)}}&{\in}\mathcal{F}.
\end{align}
To show the equality in \Cref{eq:q-liminf-limsup}, consider:
\begin{itemize}
\item ``\(\subseteq\)'':  Fix any \(\omega\in\Omega\) with
\(\liminf_{n\to\infty}X_n(\omega)<\limsup_{n\to\infty}X_n(\omega)\). By
the density of \(\Q\) on \(\R\), there exists \(q\in\Q\) such that
\(\liminf_{n\to\infty}X_n(\omega)<q<\limsup_{n\to\infty}X_n(\omega)\). Thus
\(\omega\in\bigcup_{q\in\Q}^{}\left\{\omega\in\Omega:\liminf_{n\to\infty}X_n(\omega)<
q<\limsup_{n\to\infty}X_n(\omega)\right\}\).
\item ``\(\supseteq\)'': Fix any \(\omega\in\Omega\) with
\(\liminf_{n\to\infty}X_n(\omega)<
q<\limsup_{n\to\infty}X_n(\omega)\) for some \(q\in\Q\). Then it is immediate that
\(\liminf_{n\to\infty}X_n(\omega)<\limsup_{n\to\infty}X_n(\omega)\).
\end{itemize}
\end{enumerate}
\end{pf}
\item \label{it:complete-meas-relate} \emph{(relationship between completeness and
measurability)} Let \((\Omega,\mathcal{F},\mu)\) be a measure space, and
\(X,Y,Z,X_1,X_2,\dotsc\) be functions from \(\Omega\) to \(\R\). Then:
\begin{enumerate}
\item \(\mu\) is complete iff \(\text{(\(X\) is \(\mathcal{F}\)-measurable and
\(Y\eqae X)\implies Y\) is \(\mathcal{F}\)-measurable}\).
\item \(\mu\) is complete iff \(\text{(\(X_n\) is \(\mathcal{F}\)-measurable \(\forall n\in\N\) and
\(Z\eqae \lim_{n\to \infty}X_n)\implies Z\) is \(\mathcal{F}\)-measurable}\).
\end{enumerate}

\begin{pf}
\begin{enumerate}
\item ``\(\Rightarrow\)'': Assume \(\mu\) is complete. Then \(\mathcal{F}\)
contains all subsets of every null set. Suppose \(X\) is
\(\mathcal{F}\)-measurable and \(X\eqae Y\). Then we have \(X=Y\) on \(N^c\),
where \(N\in\mathcal{F}\) is a null set.

Fix any set \(A\in\mathcal{B}(\R)\), and write \(Y^{-1}(A)=(Y^{-1}(A)\cap
N)\cup(Y^{-1}(A)\cap N^c)\). Note that:
\begin{itemize}
\item \(Y^{-1}(A)\cap N\in\mathcal{F}\) since it is a subset of the null set \(N\).
\item \(Y^{-1}(A)\cap N^c\in\mathcal{F}\) since we have \(X=Y\) on \(N^c\),
thus
\[Y^{-1}(A)\cap N^c=\underbrace{X^{-1}(A)}_{\in\mathcal{F}\text{ as
\(X\in\mathcal{F}\)}}\cap \underbrace{N^c}_{\in\mathcal{F}\text{ as
\(N\in\mathcal{F}\)}}\in\mathcal{F}.\]
\end{itemize}
Therefore, \(Y^{-1}(A)\in\mathcal{F}\), and so \(Y\) is \(\mathcal{F}\)-measurable.

``\(\Leftarrow\)'': Assume we have \(\text{(\(X\) is \(\mathcal{F}\)-measurable and
\(Y\eqae X)\implies Y\) is \(\mathcal{F}\)-measurable}\). Fix any null set
\(N\in\mathcal{F}\), and consider any subset \(N'\subseteq N\). Let
\(X=\indic_{N}\) and \(Y=\indic_{N'}\). By construction, we have \(X=Y=0\) on
\(N^c\), thus \(X\eqae Y\). Furthermore, \(X\) is \(\mathcal{F}\)-measurable
(it is a simple random variable). Hence by assumption, \(Y\) is
\(\mathcal{F}\)-measurable, meaning that \(N'=Y^{-1}(\{1\})\in\mathcal{F}\).
So we conclude that \(\mu\) is complete.
\item ``\(\Rightarrow\)'': Assume \(\mu\) is complete. Suppose \(X_n\) is
\(\mathcal{F}\)-measurable for all \(n\in\N\) and \(Z\eqae
\lim_{n\to\infty}X_n\). Then we have \(Z(\omega)=\lim_{n\to\infty}X_n(\omega)\)
for all \(\omega\in N^c\), where \(N\in\mathcal{F}\) is a null set.
Since \(\indic_{N^c}\) is \(\mathcal{F}\)-measurable, by
\labelcref{it:meas-fn-ran-vec} we know \(X_n\indic_{N^c}\) is
\(\mathcal{F}\)-measurable for all \(n\in\N\).

We claim that \(\lim_{n\to\infty}X_n(\omega)\indic_{N^c}(\omega)=Z(\omega)\indic_{N^c}(\omega)\)
for all \(\omega\in\Omega\). To see this, consider:
\begin{itemize}
\item \emph{Case 1: \(\omega\in N\).} Then we have \(\indic_{N^c}(\omega)=0\),
so both sides of the equality are zero.
\item \emph{Case 2: \(\omega\in N^c\).} The equality holds as we have
\(Z(\omega)=\lim_{n\to\infty}X_n(\omega)\) for all \(\omega\in N^c\).
\end{itemize}
Thus, by \labelcref{it:seq-rvs-meas}, \(Z\indic_{N^c}\) is
\(\mathcal{F}\)-measurable. Since \(Z\eqae Z\indic_{N^c}\) (we have
\(Z_n=Z_n\indic_{N^c}\) on \(N^c\)), by the ``\(\Rightarrow\)'' direction of
(i), we conclude that \(Z\) is \(\mathcal{F}\)-measurable.

``\(\Leftarrow\)'': Assume we have \(\text{(\(X_n\) is
\(\mathcal{F}\)-measurable \(\forall n\in\N\) and \(Z\eqae \lim_{n\to
\infty}X_n)\implies Z\) is \(\mathcal{F}\)-measurable}\). Like (i), fix any
null set \(N\in\mathcal{F}\), and consider any subset \(N'\subseteq N\). Let
\(X_n=\indic_{N}\) for all \(n\in\N\), and \(Z=\indic_{N'}\). Knowing that
\(X_n\) is \(\mathcal{F}\)-measurable for all \(n\in\N\) and
\(\lim_{n\to\infty}X_n =\indic_{N}\eqae \indic_{N'}=Z\), by assumption \(Z\) is
\(\mathcal{F}\)-measurable. Thus, \(N'=Z^{-1}(\{1\})\in\mathcal{F}\), as desired.
\end{enumerate}
\end{pf}
\end{enumerate}
\end{enumerate}
\subsection{Distributions}
\begin{enumerate}
\item In your first probability course, you have always seen terms like
``distributions of random variables'', like normal distribution, exponential
distribution, etc. Intuitively, \emph{distribution} is a concept that describes
the ``probabilistic behaviour'' of random variables. Here we shall investigate
the \emph{formal} definition of distribution, which indeed aligns with this
intuition.
\item \label{it:meas-fn-dist} \textbf{Measures induced by measurable functions.}
Let \((\Omega,\mathcal{F},\mu)\) be a measure space, \((\Omega',\mathcal{F}')\)
be a measurable space, and \(X:\Omega\to\Omega'\) be a measurable function.
Then, \(\mu_X:=\mu\circ X^{-1}\) is a measure on \((\Omega',\mathcal{F}')\).

\begin{note}
Here ``\(X^{-1}\)'' is referring to the \emph{preimage function} rather
than the inverse of \(X\), i.e., the function
\(X^{-1}:\mathcal{F}'\to\mathcal{F}\) that maps the \emph{set}
\(A'\in\mathcal{F}'\) to the \emph{preimage} \(X^{-1}(A')\in\mathcal{F}\).
\end{note}

\begin{pf}
\begin{enumerate}[label={(\arabic*)}]
\item Since \(\mu_X\) is a function from \(\mathcal{F}'\) to \([0,\infty]\), nonnegativity is satisfied.
\item We have \(\mu_X(\varnothing)=\mu(X^{-1}(\varnothing))
\overset{(X^{-1}(\varnothing)=\varnothing)}{=}\mu(\varnothing)=0\).
\item Fix any pairwise disjoint \(A_1',A_2',\dotsc\in\mathcal{F}'\). Note that
\(X^{-1}(A_1'),X^{-1}(A_2'),\dotsc\in\mathcal{F}\) are still pairwise disjoint
since for all \(i\ne j\), we have \(X^{-1}(A_i')\cap X^{-1}(A_j')
\overset{\text{\labelcref{it:preimg-prop}}}{=}X^{-1}(A_i'\cap
A_j')=X^{-1}(\varnothing)=\varnothing\). Thus,
\[
\mu_X\left(\biguplus_{i=1}^{\infty}A_i'\right)
=\mu\left(X^{-1}\left(\biguplus_{i=1}^{\infty}A_i'\right)\right)
\overset{\text{\labelcref{it:preimg-prop}}}{=}
\mu\left(\biguplus_{i=1}^{\infty}X^{-1}(A_i')\right)
=\sum_{i=1}^{\infty}\mu(X^{-1}(A_i'))
=\sum_{i=1}^{\infty}\mu_X(A_i).
\]
\end{enumerate}
\end{pf}

\begin{remark}
\item The measure \(\mu_X\) is also known as the \defn{distribution} of \(X\), or
\defn{push-forward measure}/\defn{image measure} of \(\mu\) with respect to
\(X\) (\(\mu_X\) is assigning measures to subsets of the \emph{codomain}
\(\Omega'\) rather than the domain \(\Omega\), so the measure is
``\underline{pushed forward}'').
\item A shorthand notation that is often used for distribution is as follows:
\(\mu(X\in A'):=\mu(\{\omega\in\Omega:X(\omega)\in
A'\}=\mu(X^{-1}(A'))=\mu_X(A')\).  (Usually the ``\(\mu\)'' here is \(\pr\).)
\item In case \(\mu=\pr\) is a probability measure and
\((\Omega',\mathcal{F}')=(\R^d,\mathcal{B}(\R^d))\), we have
\[
\prob{\vect{a}<\vect{X}\le\vect{b}}:=\prob{\vect{X}\in(\vect{a},\vect{b}]}
=\prob{\vect{X}^{-1}((\vect{a},\vect{b}])}
=\pr_{\vect{X}}((\vect{a},\vect{b}]),
\]
for all \(\vect{a}<\vect{b}\). Also, the distribution \(\pr_{\vect{X}}\) is a
probability measure on \(\mathcal{B}(\R^d)\) since
\(\pr_{\vect{X}}(\R^d)=\prob{X^{-1}(\R^d)}=\prob{\Omega}=1\).
\end{remark}
\item \textbf{Characterization of distribution by distribution function.}
Due to the presence of the word ``distribution'' in the term \emph{distribution
function}, one would naturally anticipate that there should be a relationship
between the concepts of \emph{distribution} and \emph{distribution function}.
This is indeed the case, and distribution function actually
\emph{characterizes} distribution (i.e., they have a one-to-one
correspondence), as the following result suggests.
\begin{theorem}[Characterization of distribution by distribution function]
\label{thm:dist-fn-char-dist}
\hfill
\begin{enumerate}
\item \emph{\(\pr_{\vect{X}}\) induces \(F\):} Let \(\vect{X}\) be a random
vector with the distribution \(\pr_{\vect{X}}\). Then, the function
\(\orc{F(\vect{x}):=\pr_{\vect{X}}((\vect{-\infty},\vect{x}])}=\prob{\vect{X}\le\vect{x}}\)
is the distribution function of \(\pr_{\vect{X}}\).
\item \emph{\(F\) induces \(\pr_{\vect{X}}\):} If \(F\) is a distribution
function on \(\R^d\), then we can define a probability space
\((\Omega,\mathcal{F},\pr)\) such that there is a random vector \(\vect{X}:\Omega\to\R^d\)
with distribution \(\pr_{\vect{X}}\) that satisfies \(\vc{\pr_{\vect{X}}((\vect{-\infty},\vect{x}])}
=\prob{\vect{X}\le\vect{x}}\vc{=F(\vect{x})}\) for all \(\vect{x}\in\R^d\).
\end{enumerate}
\end{theorem}
\begin{pf}
\begin{enumerate}
\item From \labelcref{it:meas-fn-dist} we know that
\(\pr_{\vect{X}}\) is a probability measure on \(\mathcal{B}(\R^d)\).
Therefore, by \Cref{thm:dist-fn-char-prob-meas-rd}, the function
\(F(\vect{x}):=\pr_{\vect{X}}((\vect{-\infty},\vect{x}])=\prob{\vect{X}\le\vect{x}}\)
is the distribution function of \(\pr_{\vect{X}}\).
\item Here we only prove the case for \(d=1\). Assume \(F\) is a distribution
function on \(\R\). Letting
\((\Omega,\mathcal{F},\pr)=((0,1],\mathcal{B}((0,1]),\lambda)\) (where
\(\lambda\) is the Lebesgue measure, with domain restricted to
\(\mathcal{B}((0,1])\)) and \(X:=F^{-1}\), we have
\begin{align*}
\prob{X\le x}&=\prob{\{\omega\in\Omega:X(\omega)\le x\}}
=\prob{\{\omega\in\Omega:F^{-1}(\omega)\le x\}} \\
\overset{\text{\labelcref{it:gen-inv-prop}}}&{=}
\prob{\{\omega\in\Omega:\omega\le F(x)\}}
=\prob{(0,F(x)]}
=\lambda((0,F(x)])
=F(x)
\end{align*}
for all \(x\in\R\).
\end{enumerate}
\end{pf}
\begin{center}
\begin{tikzpicture}
\node[scale=2] () at (0,0) {\(\pr_{\vect{X}}\)};
\node[scale=2] () at (5,0) {\(F\)};
\node[scale=2] () at (10,0) {\(\pr_{\vect{X}}\)};
\draw[-Latex] (0.5,0) --node[midway,above]{\orc{\(F(\vect{x}):=\pr_{\vect{X}}((\vect{-\infty},\vect{x}])\)}} (4.5,0);
\draw[-Latex] (5.5,0) --node[midway,above]{\(\vc{\pr_{\vect{X}}((\vect{-\infty},\vect{x}])=F(\vect{x})}\)} (9.5,0);
\end{tikzpicture}
\begin{tikzpicture}
\node[scale=2] () at (0,0) {\(F\)};
\node[scale=2] () at (5,0) {\(\pr_{\vect{X}}\)};
\node[scale=2] () at (10,0) {\(F\)};
\draw[-Latex] (0.5,0) --node[midway,above]{\(\vc{\pr_{\vect{X}}((\vect{-\infty},\vect{x}])=F(\vect{x})}\)} (4.5,0);
\draw[-Latex] (5.5,0) --node[midway,above]{\orc{\(F(\vect{x}):=\pr_{\vect{X}}((\vect{-\infty},\vect{x}])\)}} (9.5,0);
\end{tikzpicture}
\end{center}
\begin{remark}
\item In view of this result, such distribution function \(F\) is said to be
the \defn{distribution function of \(\vect{X}\)} (indeed this is perhaps how
distribution function is \emph{defined} in your first probability course),
denoted by \(\vect{X}\sim F\), and sometimes we write the distribution function
as \(F_{\vect{X}}\).

\item By \Cref{prp:meas-unique,prp:borel-rd-generators}, it can be shown that
for two probability measures \(\pr\) and \(\Q\) on
\(\mathcal{B}(\R^d)\), if
\(\prob{(\vect{-\infty},\vect{x}]}=\Q((\vect{-\infty},\vect{x}])\) for
all \(\vect{x}\in\R^d\), then \(\pr=\Q\). This explains why the picture above
depicts a one-to-one correspondence between \(F\) and \(\pr_{\vect{X}}\):
For the first line, applying (b) on the \(F\) induced by \(\pr_{\vect{X}}\)
would indeed give you back the original \(\pr_{\vect{X}}\), as the resulting
probability measure agrees with the original \(\pr_{\vect{X}}\) for every set
of the form \((\vect{-\infty},\vect{x}]\), and so coincides with \(\pr_{\vect{X}}\).

\item If two random vectors \(\vect{X}_1\) and
\(\vect{X}_2\), defined on \((\Omega_1,\mathcal{F}_1,\pr_{1})\) and
\((\Omega_2,\mathcal{F}_2,\pr_{2})\) respectively, have the same distribution
function, then their distributions \((\pr_1)_{\vect{X}_1}\) and
\((\pr_2)_{\vect{X}_2}\) would be the same as well. This leads to the following
definition: Two random vectors \(\vect{X}_1\) and \(\vect{X}_2\) are said to be
\defn{equal in distribution}, denoted by \(\vect{X}_1\eqd\vect{X}_2\), if we
have \(\vect{X}_1,\vect{X}_2\sim F\) (which would indeed imply that
\(\vect{X}_1\) and \(\vect{X}_2\) have the same distribution, as the name
suggests).

\item For a random vector \(\vect{X}\sim F\), we have
\[
\prob{\vect{X}\in(\vect{a},\vect{b}]}=\pr_{\vect{X}}((\vect{a},\vect{b}])
\overset{\text{(\Cref{thm:dist-fn-char-prob-meas-rd})}}{=}\lambda_{F}((\vect{a},\vect{b}])
=\Delta_{(\vect{a},\vect{b}]}F,
\]
offering another interpretation of the \(F\)-volume
\(\Delta_{(\vect{a},\vect{b}]}F\): the probability for \(\vect{X}\sim F\) to
take values in \((\vect{a},\vect{b}]\).
\item A random vector \(\vect{X}\) is said to be \defn{discrete},
\defn{absolutely continuous}, \defn{continuous singular}, or \defn{mixed type}
if its distribution function is. Since density/mass function \(f\) uniquely
determines the distribution function \(F\), sometimes we also write
\(\vect{X}\sim f\) to mean \(\vect{X}\sim F\), whenever the density/mass
function \(f\) exists. This applies similarly to other expressions that uniquely
characterize \(F\) also, e.g., we write ``\(X\sim\ndist{\mu,\sigma^2}\)'' as
the expression ``\(\ndist{\mu,\sigma^2}\)'' refers to a unique distribution function.
\item When we have \(\vect{X}\sim F\), sometimes we call
\(\supp{F}=\{\vect{x}\in\R^d:\Delta_{(\vect{x}-\vect{h},\vect{x}]}F>0\text{ for
all }\vect{h}>\vect{0}\}\) as the \defn{support} of \(\vect{X}\), denoted by
\(\supp{\vect{X}}\). There are some other alternative definitions of the
support of \(\vect{X}\), including:
\begin{itemize}
\item the \emph{smallest closed set \(C\subseteq \R^d\)} such that
\(\prob{\vect{X}\in C}=1\), and
\item the set \(\{\vect{x}\in\R^d:\pr_{\vect{X}}(B(\vect{x},r))>0\text{ for
all }r>0\}\) where \(B(\vect{x},r)\) denotes the open ball in \(\R^d\) with
center \(\vect{x}\) and radius \(r\).
\end{itemize}
It can be shown that all these definitions are equivalent; see
\url{https://math.stackexchange.com/q/846011} for a related discussion on this
matter.
\item Sometimes one abuses the notation of \(F\) and writes
\(F(B):=\pr_{\vect{X}}(B)\). In view of this, the notations \(F\) and
\(\pr_{\vect{X}}\) are sometimes used interchangeably (see e.g.,
\labelcref{it:lebesgue-stieljes-int}). Also, the terms ``distribution'' and
``distribution function'' are used interchangeably sometimes.
\end{remark}
\item \textbf{Preservation of equality in distribution after applying
measurable functions.} Intuitively, given two random vectors that are equal in
distribution, applying an identical (measurable) function to each of them
should still preserve the equality in distribution, as the function should lead
to the same ``change'' for both distributions, so the resulting ones would
still be the same as each other. The following result justifies this intuition.
\begin{proposition}
\label{prp:eqd-preserv-after-meas-fun}
Let \(\vect{X}_1\) and \(\vect{X}_2\) be random vectors on
\((\Omega_1,\mathcal{F}_1,\pr_{1})\) and \((\Omega_2,\mathcal{F}_2,\pr_{2})\)
respectively.  If \(\vect{X}_1\eqd \vect{X}_2\), then
\(\vect{h}(\vect{X}_1)\eqd\vect{h}(\vect{X}_2)\) for every measurable function
\(\vect{h}:\R^d\to\R^k\).
\end{proposition}
\begin{pf}
For every measurable function \(\vect{h}:\R^d\to\R^k\), we have
\begin{align*}
F_{\vect{h}(\vect{X}_1)}(\vect{x})
&=\pr_{1}(\vect{h}(\vect{X}_1)\le\vect{x})
=\pr_{1}(\vect{X}_1\in\vect{h}^{-1}((\vect{-\infty},\vect{x}]) \\
\overset{(\vect{X}_1\eqd \vect{X}_2)}&{=}
\pr_{2}(\vect{X}_2\in\vect{h}^{-1}((\vect{-\infty},\vect{x}])
=\pr_{2}(\vect{h}(\vect{X}_2)\le\vect{x})
=F_{\vect{h}(\vect{X}_2)}(\vect{x})
\end{align*}
for all \(\vect{x}\in\R^k\). Thus,
\(\vect{h}(\vect{X}_1)\eqd\vect{h}(\vect{X}_2)\) by definition.
\end{pf}
\end{enumerate}
\subsection{Margins}
\begin{enumerate}
\item \textbf{Margins of random vectors.} Recall from
\labelcref{it:dist-fn-fmla} that the \emph{\(J\)-margin} of \(F\) is given by
\(F_J(\vect{x}_j):=\lim_{\vect{x}_{J^c}\to\vect{\infty}}F(\vect{x})\).  Here,
we consider the special case where \(F\) is the distribution function of a
random vector \(\vect{X}\). In such case, the \(J\)-margin of \(F\) is given by
\[
F_J(\vect{x}_j)=\lim_{\vect{x}_{J^c}\to\vect{\infty}}\prob{\vect{X}\le\vect{x}}
\overset{\text{(continuity from below)}}{=}
=\prob{\vect{X}_J\le\vect{x}_J}
\]
for all \(\vect{x}_J\in\R^{|J|}\). \begin{note}
Here we view \(\vect{X}\) as the vector \((X_1,\dotsc,X_d)\), so the
notation \(\vect{X}_{J}\) refers to the vector \((X_j)_{j\in J}\).
\end{note}

By \Cref{thm:dist-fn-char-dist}, we know that this is the distribution
function of \(\vect{X}_J\). In view of this, the random vector \(\vect{X}_{J}\)
is called the \defn{\(J\)-margin of \(\vect{X}\)}. In the special case where
\(J=\{j\}\) for some \(j=1,\dotsc,d\), we would have the \emph{\(j\)th margin
of \(F\)}, given by \(F_j(x_j)=\prob{X_j\le x_j}\), and hence \(X_j\) is called
the \defn{\(j\)th margin of \(\vect{X}\)}.
\item \textbf{Margins of absolutely continuous/discrete random vectors.}
First consider the special case where \(F\) is absolutely continuous. The
\(J\)-margin of \(F\) would then be
\[
F_J(\vect{x}_J)=F(\vect{\infty}_{J\leftarrow\vect{x}_{J}})
=\int_{(\vect{-\infty},\vect{\infty}_{J\leftarrow\vect{x}_{J}}]}^{}f(\vect{z})\odif{\vect{z}}
=\int_{-\vect{\infty}}^{\vect{x}_{J}}\vc{\int_{\vect{-\infty}}^{\vect{\infty}}f(\vect{z})
\odif{\vect{z}_{J^c}}}\odif{\vect{z}_{J}},
\quad\text{for all \(\vect{x}_{J}\in\R^{|J|}\)}.
\]
Thus, \(F_J\) is also absolutely continuous with the density function being
\(f_J(\vect{x}_J)=\vc{\int_{\vect{-\infty}}^{\vect{\infty}}f(\vect{z})
\odif{\vect{z}_{J^c}}}\), which is called the \defn{\(J\)-marginal density
function} of \(F\) or \(\vect{X}\). This means that every lower-dimensional
margin of an absolutely continuous distribution function \(F\) is absolutely
continuous, and the corresponding density function can be obtained by
\emph{integrating out the joint density \(f\)} over the other variables.
For the case where \(F\) is discrete, change \(\int\to\sum\),
\(\text{``integrating''}\to\text{``summing''}\), and
\(\text{``density''}\to\text{``mass''}\).

However, having absolutely continuous margins do \emph{not} imply that \(F\) is
absolutely continuous. To see this, consider an example where
\(\vect{X}=(U,U)\) with \(U\sim \unif{0}{1}\). Here both \(X_1=U\) and
\(X_2=U\) are absolutely continuous, with the density function being
\(f(x)=\indic_{(0,1)}(x)\). On the other hand, since the distribution function
of \(\vect{X}\) is \(F_{\vect{X}}(\vect{x})=\prob{U\le x_1, U\le
x_2}=\prob{U\le\min\{x_1,x_2\}}=\min\{x_1,x_2\}\), where \(\min\{x_1,x_2\}\in
(0,1)\), we have
\[
\pdv{}{x_2,x_1}F_{\vect{X}}(\vect{x})=
\begin{cases}
\pdv{}{x_2,x_1}x_1=\pdv{}{x_2}1=0&\text{if \(x_1<x_2\),} \\
\pdv{}{x_2,x_1}x_2=\pdv{}{x_2}0=0&\text{if \(x_1>x_2\),} \\
\end{cases}
\]
which integrates to \(0\) rather than \(1\). This then implies that \(F\) is
not absolutely continuous.
\end{enumerate}
\subsection{Survival Functions}
\begin{enumerate}
\item While distribution functions are describing probabilities of the form
\(\prob{\vect{X}\le\vect{x}}\), \emph{survival distributions} are describing probabilities
of the form \(\prob{\vect{X}>\vect{x}}\), which are \emph{exceedance
probabilities}. Such probabilities are of great interest in some applications,
such as modelling extreme losses in actuarial science, where \(\vect{X}\) could
represent a collection of losses.

\item \textbf{Definitions.} The \defn{survival function} of a random vector
\(\vect{X}\sim F\) is defined by
\(\bar{F}(\vect{x}):=\prob{\vect{X}>\vect{x}}\) for all \(\vect{x}\in\R^d\).
The \defn{\(J\)-margin of \(\bar{F}\)} is given by
\(\bar{F}_{J}(\vect{x})=\prob{\vect{X}_{J}>\vect{x}_J}\), and the \(j\)th
marginal survival function is \(\bar{F}_j(x_j):=\prob{X_j>x_j}=1-F_j(x_j)\),
for all \(j=1,\dotsc,d\).

\item \label{it:surv-expr-dist-fn} \textbf{Expression of survival functions in terms of distribution
functions.} While we have the ``nice'' relationship \(\bar{F}(x)=1-F(x)\) when
\(d=1\), it breaks down when \(d\ge 2\). For instance, with \(d=2\) we have
\(\bar{F}(x_1,x_2)=\prob{X_1>x_1\cap X_2>x_2}
=1-\prob{X_1\le x_1\cup X_2\le x_2}
\overset{\text{(inclusion-exclusion)}}{=}1-(F_1(x_1)+F_2(x_2)-F(x_1,x_2))
=1-F_1(x_1)-F_2(x_2)+F(x_1,x_2)\), which is clearly \emph{not} equal to
\(1-F(x_1,x_2)\) in general \warn{}.

In general, the expression of \(\bar{F}\) in terms of \(F\) is as follows:
\[
\bar{F}(\vect{x})=\sum_{J\subseteq \{1,\dotsc,d\}}^{}(-1)^{|J|}F(\vect{\infty}_{J\leftarrow\vect{x}_{J}})
\]
for all \(\vect{x}\in\R^d\).

\begin{pf}
We again utilize the inclusion-exclusion principle. First let \(S_{j,d}:=
\sum_{J\subseteq \{1,\dotsc,d\}:|J|=j}^{}\prob{\bigcap_{k\in J}^{}\{X_k\le
x_k\}}\) for every \(j=0,1,\dotsc\), with the convention that
\(\bigcap_{k\in\varnothing}^{}\{X_k\le x_k\}:=\Omega\). Then,
\begin{align*}
\bar{F}(\vect{x})&=\prob{\vect{X}>\vect{x}}=1-\prob{\bigcup_{j=1}^{d}\{X_j\le x_j\}} \\
\overset{\text{(inclusion-exclusion)}}&{=}1-\sum_{j=1}^{d}(-1)^{j-1}S_{j,d}
=1+\sum_{j=1}^{d}(-1)^{j}S_{j,d} \\
&=\sum_{j=0}^{d}(-1)^{j}S_{j,d}
=\sum_{j=0}^{d}(-1)^{j}\sum_{J\subseteq \{1,\dotsc,d\}:|J|=j}^{}\prob{\bigcap_{k\in J}^{}\{X_k\le x_k\}} \\
&=\sum_{j=0}^{d}(-1)^{j}\sum_{J\subseteq \{1,\dotsc,d\}:|J|=j}^{}F(\vect{\infty}_{J\leftarrow \vect{x}_{J}})
=\sum_{J\subseteq \{1,\dotsc,d\}}^{}(-1)^{|J|}F(\vect{\infty}_{J\leftarrow\vect{x}_{J}}).
\end{align*}
\end{pf}
\end{enumerate}
\subsection{Stochastic Processes}
\begin{enumerate}
\item Similar to \labelcref{it:ran-vec-vec-rvs}, we actually have that \(X\) is
a random sequence iff \(X=(X_1,X_2,\dotsc)\) where \(X_1,X_2,\dotsc\) are
random variables. Such \(X\) is also known as a \defn{discrete-time stochastic
process}.
\begin{note}
Here \((X_1,X_2,\dotsc)\) is actually referring to the sequence \(\{X_n\}_{n\in\N}\).
\end{note}

But apart from \emph{discrete-time} stochastic process, you should have
previously learnt also \emph{continuous-time} stochastic processes, e.g.,
Brownian motion. While it appears that a continuous-time stochastic process can
be obtained by just modifying the index set appropriately, the work it takes to
justify the sensibility of such modification turns out to be highly
non-trivial; it indeed requires a technical result known as
\emph{Kolmogorov's extension theorem}, which can be proved by the
Carath\'eodory's extension theorem. In the following, we will state the theorem
without proof.

\item \textbf{Kolmogorov's extension theorem.} The \emph{Kolmogorov's extension
theorem} allows us to \underline{extend} from the discrete-time stochastic
process \(\{X_t\}_{t\in\N}\) to a very general stochastic process
\(\{\vect{X}_t\}_{t\in I}\), where \(I\subseteq \R\) is an index set, and
\(\vect{X}_t:\Omega\to\R^d\) is a random vector for each \(t\in I\).

Before stating the theorem, we first introduce a preliminary notion. Fix any
\(k\in\N\) and \(t_1,\dotsc,t_k\in I\) with \(t_1<\dotsb<t_k\). Then the
probability measure \(\pr_{t_1,\dotsc,t_k}\) on \(\mathcal{B}(\R^d)^{k}\) given
by \(\pr_{t_1,\dotsc,t_k}(\prod_{i=1}^{k}B_i) :=\prob{\vect{X}_{t_1}\in
B_1,\dotsc,\vect{X}_{t_k}\in B_k}\) for all
\(B_1,\dotsc,B_k\in\mathcal{B}(\R^d)\) is said to be a \defn{finite-dimensional
distribution} of \(\{\vect{X}_t\}_{t\in I}\).

\begin{theorem}[Kolmogorov's extension theorem]
\label{thm:kolmogorov-extension}
For all \(k\in\N\) and all \(t_1,\dotsc,t_k\in I\) with \(t_1<\dotsb<t_k\),
suppose that there is a probability measure \(\pr_{t_1,\dotsc,t_k}\) on
\(\mathcal{B}(\R^d)^{k}\) satisfying:
\begin{itemize}
\item \emph{(permutation invariance)} \(\pr_{t_{\pi(1)},\dotsc,t_{\pi(k)}}(\prod_{i=1}^{k}B_{\pi(i)})
=\pr_{t_1,\dotsc,t_k}(\prod_{i=1}^{k}B_i)\) for all
\(B_1,\dotsc,B_k\in\mathcal{B}(\R^d)\) and all permutations \(\pi\) of \(\{1,\dotsc,k\}\).
\item \emph{(compatibility)} \(\pr_{t_1,\dotsc,t_{k+1}}(\prod_{i=1}^{k}B_k\times\R^d)
=\pr_{t_1,\dotsc,t_{k+1}}(\prod_{i=1}^{k}B_i)\) for all
\(B_1,\dotsc,B_k\in\mathcal{B}(\R^d)\).
\end{itemize}
Then there exists a probability space
\(((\R^d)^{I},\mathcal{B}(\R^d)^{I},\pr)\)\footnote{The notation \(S^I\)
denotes the set of all functions from \(I\) to \(S\).} on which the
finite-dimensional distributions of \(\{\vect{X}_t\}_{t\in I}\) are given by
the \(\pr_{t_1,\dotsc,t_k}\)'s.
\end{theorem}
\begin{pf}
Omitted.
\end{pf}

In short, the Kolmogorov's extension theorem asserts that we can construct a
general stochastic process \(\{\vect{X}_t\}_{t\in I}\) by specifying its
probabilistic behaviour at finitely many time points \(t_1,\dotsc,t_k\),
through the probability measure \(\pr_{t_1,\dotsc,t_k}\), as long as such
specification is sufficiently ``well-behaved''. It provides the theoretical
underpinning for studies of stochastic processes that are more general than
just discrete-time stochastic processes.
\begin{note}
The famous \emph{Brownian motion} is obtained by specifying the probability
measures \(\pr_{t_1,\dotsc,t_k}\) that correspond to multivariate normal
distributions.
\end{note}
\end{enumerate}
\subsection{Univariate Transforms}
\label{subsect:univariate-trans}
\begin{enumerate}
\item To close \Cref{sect:meas-fn}, we will introduce some results about
\emph{transformations} of random variables, which are helpful for
performing \emph{simulations} (see \labelcref{it:sampling}).
\item \textbf{Quantile transform.} The first results suggests the distribution
of a \(\unif{0}{1}\) random variable after being transformed by a
\underline{quantile} function \(F^{-1}\).

\begin{proposition}[Quantile transform]
\label{prp:quantile-trans}
Let \(F\) be a distribution function and \(U\sim\unif{0}{1}\). Then
\(F^{-1}(U)\sim F\).
\end{proposition}
\begin{pf}
Note that \(\prob{F^{-1}(U)\le x}
\overset{\text{(\labelcref{it:gen-inv-prop}; \(F\) is right-continuous)}}{=}
\prob{U\le F(x)}=F(x)\) for all \(x\in\R\).
\end{pf}
\item \textbf{Probability transform.} Apart from the quantile transform, there
is also a result that works another way round, but it requires a continuity
assumption. The following lemma is used for proving the result.

\begin{lemma}[Strict increasingness of an univariate distribution function on its support]
\label{lma:strict-incr-support}
Let \(F\) be a distribution function on \(\R\). Then, \(F\) is strictly
increasing on its support \(\supp{F}=\{x\in\R:F(x)-F(x-h)>0\text{ for all
}h>0\}\), i.e., for all \(x,y\in\supp{F}\) with \(x<y\), we have
\(F(x)<F(y)\).
\end{lemma}
\begin{pf}
Fix any \(x,y\in\supp{F}\) with \(x<y\), and set \(h=y-x>0\). Then, by the
definition of support we have \(F(y)-F(x)=F(y)-F(y-h)>0\), so the strict
increasingness follows.
\end{pf}

\begin{proposition}[Probability transform]
\label{prp:prob-trans}
If \(X\sim F\) and \vc{\(F\) is continuous}, then \(F(X)\sim \unif{0}{1}\).
\end{proposition}
\begin{pf}
Since \(F\) is continuous, the quantile function \(F^{-1}\) is strictly
increasing on \([0,1]\) by \labelcref{it:gen-inv-prop}. Therefore, we have
\(F(X(\omega))\le u\) iff \(F^{-1}(F(X(\omega)))\le F^{-1}(u)\) for all
\(\omega\in\Omega\). Thus, we have
\begin{align*}
\prob{F(X)\le u}&=\prob{F^{-1}(F(X))\le F^{-1}(u)} \\
&=\underbrace{\prob{\{F^{-1}(F(X))\le F^{-1}(u)\}\cap \{X\in\supp{F}\}}}
_{\overset{\text{\labelcref{it:gen-inv-prop}}}{=}\prob{\{X\le F^{-1}(u)\}
\cap \{X\in\supp{F}\}}
}
+\underbrace{\prob{\{F^{-1}(F(X))\le F^{-1}(u)\}\cap
\{X\notin\supp{F}\}}}_{=0\text{ as \(\prob{X\in\supp{F}}=1\)}} \\
&=\prob{\{X\le F^{-1}(u)\} \cap \{X\in\supp{F}\}}+
\underbrace{\prob{\{X\le F^{-1}(u)\} \cap \{X\notin\supp{F}\}}}_{0} \\
&=\prob{X\le F^{-1}(u)}=F(F^{-1}(u)),
\end{align*}
which equals \(u\) for all \(u\in\ran{F}\cup\{0,1\}\overset{\text{(\(F\)
continuous)}}{=}[0,1]\) by \labelcref{it:gen-inv-prop}. This implies that
\(F(X)\sim\unif{0}{1}\).
\end{pf}
\end{enumerate}
